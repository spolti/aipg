kind: Dashboard
metadata:
    name: aec6hbnje0zy8a
    createdAt: 0001-01-01T00:00:00Z
    updatedAt: 0001-01-01T00:00:00Z
    version: 0
    project: ""
spec:
    display:
        name: vLLM / GPU Metrics - NVIDIA
    variables:
        - kind: ListVariable
          spec:
            display:
                hidden: false
            defaultValue: 682a2344-305f-4039-b45a-303051728943
            allowAllValue: false
            allowMultiple: false
            plugin:
                kind: StaticListVariable
                spec:
                    values:
                        - grafana
                        - migration
                        - not
                        - supported
            name: datasource
        - kind: ListVariable
          spec:
            display:
                hidden: false
            defaultValue:
                - $__all
            allowAllValue: true
            allowMultiple: true
            sort: alphabetical-asc
            plugin:
                kind: PrometheusLabelValuesVariable
                spec:
                    labelName: instance
                    matchers:
                        - DCGM_FI_DEV_GPU_TEMP
            name: instance
        - kind: ListVariable
          spec:
            display:
                hidden: false
            defaultValue:
                - $__all
            allowAllValue: true
            allowMultiple: true
            sort: alphabetical-asc
            plugin:
                kind: PrometheusLabelValuesVariable
                spec:
                    labelName: gpu
                    matchers:
                        - DCGM_FI_DEV_GPU_TEMP
            name: gpu
        - kind: TextVariable
          spec:
            display:
                hidden: true
            value: ${NAMESPACE}
            name: namespace
        - kind: TextVariable
          spec:
            display:
                hidden: true
            value: ${MODEL_NAME}
            constant: true
            name: model_name
    panels:
        "0_0":
            kind: Panel
            spec:
                display:
                    name: GPU Temperature
                    description: GPU Current Temperature
                plugin:
                    kind: TimeSeriesChart
                    spec:
                        legend:
                            mode: table
                            position: bottom
                            values:
                                - mean
                                - last-number
                                - max
                        visual:
                            areaOpacity: 0.1
                            connectNulls: false
                            display: line
                            lineWidth: 2
                queries:
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: $datasource
                                minStep: ""
                                query: DCGM_FI_DEV_GPU_TEMP{instance=~"$instance", gpu=~"$gpu"}
                                seriesNameFormat: GPU {{gpu}}
        "0_1":
            kind: Panel
            spec:
                display:
                    name: GPU Avg. Temp
                    description: Average GPU temerature
                plugin:
                    kind: GaugeChart
                    spec:
                        calculation: mean
                        max: 100
                        thresholds:
                            steps:
                                - color: green
                                  value: 0
                                - color: '#EAB839'
                                  value: 83
                                - color: red
                                  value: 87
                queries:
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: ${datasource}
                                query: avg(DCGM_FI_DEV_GPU_TEMP{instance=~"$instance", gpu=~"$gpu"})
                                seriesNameFormat: '{{name}} ({{condition}})'
        "0_2":
            kind: Panel
            spec:
                display:
                    name: GPU SM Clocks
                    description: |-
                        Key Metrics to Visualize:
                            - GPU Temperature: Monitor the GPU temperature. Throttling often occurs when the GPU reaches a certain temperature (e.g., 85-90Â°C)2.
                            - SM Clock Speed: Observe the core clock speed. A significant drop in the clock speed while the GPU is under load indicates throttling.
                plugin:
                    kind: TimeSeriesChart
                    spec:
                        legend:
                            mode: table
                            position: bottom
                            values:
                                - mean
                                - last-number
                                - max
                        visual:
                            areaOpacity: 0.1
                            connectNulls: false
                            display: line
                            lineWidth: 2
                queries:
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: $datasource
                                minStep: ""
                                query: DCGM_FI_DEV_SM_CLOCK{instance=~"$instance", gpu=~"$gpu"} * 1000000
                                seriesNameFormat: GPU {{gpu}}
        "0_3":
            kind: Panel
            spec:
                display:
                    name: GPU Power Usage
                    description: GPU Current Power Usage
                plugin:
                    kind: TimeSeriesChart
                    spec:
                        legend:
                            mode: table
                            position: bottom
                            values:
                                - mean
                                - last-number
                                - max
                        visual:
                            areaOpacity: 0.1
                            connectNulls: false
                            display: line
                            lineWidth: 2
                queries:
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: $datasource
                                minStep: ""
                                query: DCGM_FI_DEV_POWER_USAGE{instance=~"$instance", gpu=~"$gpu"}
                                seriesNameFormat: GPU {{gpu}}
        "0_4":
            kind: Panel
            spec:
                display:
                    name: GPU Power Total
                    description: GPU Total Power - The SUM of the power read from all sensors
                plugin:
                    kind: GaugeChart
                    spec:
                        calculation: sum
                        max: 2400
                        thresholds:
                            steps:
                                - color: green
                                  value: 0
                                - color: '#EAB839'
                                  value: 1800
                                - color: red
                                  value: 2200
                queries:
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: ${datasource}
                                query: sum(DCGM_FI_DEV_POWER_USAGE{instance=~"$instance", gpu=~"$gpu"})
                                seriesNameFormat: '{{name}} ({{condition}})'
        "0_5":
            kind: Panel
            spec:
                display:
                    name: GPU Utilization
                    description: Current GPU Utilization
                plugin:
                    kind: TimeSeriesChart
                    spec:
                        legend:
                            mode: table
                            position: bottom
                            values:
                                - mean
                                - last-number
                                - max
                        visual:
                            areaOpacity: 0.1
                            connectNulls: false
                            display: line
                            lineWidth: 2
                        yAxis:
                            format:
                                unit: percent
                            max: 100
                            min: 0
                queries:
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: $datasource
                                minStep: ""
                                query: DCGM_FI_DEV_GPU_UTIL{instance=~"$instance", gpu=~"$gpu"}
                                seriesNameFormat: GPU {{gpu}}
        "0_6":
            kind: Panel
            spec:
                display:
                    name: CPU-GPU Bottleneck
                    description: "If CPU throttling is low and GPU utilization is high, it indicates that the system is well-balanced, with the GPU being fully utilized without CPU constraints. \n  If CPU throttling is high and GPU utilization is low, it indicates a CPU bottleneck. The CPU is unable to keep up with the GPU's processing demands, causing the GPU to remain underutilized. If both metrics are high, it may indicate that the workload is demanding for both CPU and GPU, and you may need to scale up resources."
                plugin:
                    kind: TimeSeriesChart
                    spec:
                        legend:
                            mode: table
                            position: bottom
                            values:
                                - mean
                                - last-number
                                - max
                        visual:
                            areaOpacity: 0.1
                            connectNulls: false
                            display: line
                            lineWidth: 2
                        yAxis:
                            format:
                                unit: percent
                            max: 100
                            min: 0
                queries:
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: 682a2344-305f-4039-b45a-303051728943
                                minStep: ""
                                query: sum(rate(container_cpu_cfs_throttled_seconds_total{namespace="$namespace", pod=~"$model_name.*"}[5m])) by (namespace)
                                seriesNameFormat: CPU {{cpu}}
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: 682a2344-305f-4039-b45a-303051728943
                                query: avg(DCGM_FI_DEV_GPU_UTIL{instance=~"$instance", gpu=~"$gpu"})
                                seriesNameFormat: GPU {{gpu}}
        "1_0":
            kind: Panel
            spec:
                display:
                    name: GPU / CPU Cache Utilization
                    description: Tracks the percentage of GPU memory used by the vLLM model, providing insights into memory efficiency
                plugin:
                    kind: TimeSeriesChart
                    spec:
                        legend:
                            mode: list
                            position: bottom
                            values: []
                        visual:
                            areaOpacity: 0
                            connectNulls: false
                            display: line
                            lineWidth: 1
                        yAxis:
                            format:
                                unit: percent-decimal
                queries:
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: ${datasource}
                                query: vllm:gpu_cache_usage_perc{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}
                                seriesNameFormat: GPU Cache Usage
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: ${datasource}
                                query: vllm:cpu_cache_usage_perc{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}
                                seriesNameFormat: CPU Cache Usage
        "1_1":
            kind: Panel
            spec:
                display:
                    name: GPU / CPU Cache Usage - 24h
                    description: |-
                        Tracks the percentage of GPU memory used by the vLLM model, providing insights into memory efficiency.

                        24 Hours sum over time.
                plugin:
                    kind: TimeSeriesChart
                    spec:
                        legend:
                            mode: list
                            position: bottom
                            values: []
                        visual:
                            areaOpacity: 0
                            connectNulls: false
                            display: line
                            lineWidth: 1
                        yAxis:
                            format:
                                unit: percent
                queries:
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: 682a2344-305f-4039-b45a-303051728943
                                query: sum_over_time(vllm:gpu_cache_usage_perc{namespace="${namespace}",pod=~"$model_name.*"}[24h])
                                seriesNameFormat: GPU Cache Usage
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: 682a2344-305f-4039-b45a-303051728943
                                query: sum_over_time(vllm:cpu_cache_usage_perc{namespace="${namespace}",pod=~"$model_name.*"}[24h])
                                seriesNameFormat: CPU Cache Usage
        "1_2":
            kind: Panel
            spec:
                display:
                    name: E2E Request Latency - 5m
                    description: |-
                        Measures the overall time to process a request, critical for user experience.

                        This is a `histogram_quantile(0.99, ...)`

                        It computes the 99th percentile (P99) latency from the histogram buckets.

                        This means 99% of requests had a latency less than or equal to the returned value.

                        The same goes for the other buckets
                plugin:
                    kind: TimeSeriesChart
                    spec:
                        legend:
                            mode: list
                            position: bottom
                            values: []
                        visual:
                            areaOpacity: 0
                            connectNulls: false
                            display: line
                            lineWidth: 1
                        yAxis:
                            format:
                                unit: seconds
                queries:
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: ${datasource}
                                query: histogram_quantile(0.99, sum by(le) (rate(vllm:e2e_request_latency_seconds_bucket{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])))
                                seriesNameFormat: P99
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: ${datasource}
                                query: histogram_quantile(0.95, sum by(le) (rate(vllm:e2e_request_latency_seconds_bucket{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])))
                                seriesNameFormat: P95
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: ${datasource}
                                query: histogram_quantile(0.9, sum by(le) (rate(vllm:e2e_request_latency_seconds_bucket{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])))
                                seriesNameFormat: P90
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: ${datasource}
                                query: histogram_quantile(0.5, sum by(le) (rate(vllm:e2e_request_latency_seconds_bucket{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])))
                                seriesNameFormat: P50
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: ${datasource}
                                query: |-
                                    rate(vllm:e2e_request_latency_seconds_sum{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[$__rate_interval])
                                    /
                                    rate(vllm:e2e_request_latency_seconds_count{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[$__rate_interval])
                                seriesNameFormat: Average
        "1_3":
            kind: Panel
            spec:
                display:
                    name: Running  x Waiting Requests
                    description: "Running Requests: The number of requests actively being processed; helps monitor workload concurrency.\n\nX \n\nWaiting Requests: Tracks requests in the queue, indicating system saturation.\n"
                plugin:
                    kind: TimeSeriesChart
                    spec:
                        legend:
                            mode: list
                            position: bottom
                            values: []
                        thresholds:
                            steps:
                                - color: transparent
                                  value: 0
                        visual:
                            areaOpacity: 0
                            connectNulls: false
                            display: line
                            lineWidth: 1
                        yAxis:
                            format:
                                decimalPlaces: 0
                                unit: decimal
                            min: 0
                queries:
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: 682a2344-305f-4039-b45a-303051728943
                                query: vllm:num_requests_running{namespace='${namespace}',  pod=~"$model_name.*"}
                                seriesNameFormat: Running Requests
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: 682a2344-305f-4039-b45a-303051728943
                                query: vllm:num_requests_waiting{namespace='${namespace}',  pod=~"$model_name.*"}
                                seriesNameFormat: Waiting Requests
        "1_4":
            kind: Panel
            spec:
                display:
                    name: Time To First Token Latency - 5m
                    description: |-
                        The time taken to generate the first token in a response.

                        This is a `histogram_quantile(0.99, ...)`

                        It computes the 99th percentile (P99) latency from the histogram buckets.

                        This means 99% of requests had a latency less than or equal to the returned value.

                        The same goes for the other buckets
                plugin:
                    kind: TimeSeriesChart
                    spec:
                        legend:
                            mode: list
                            position: bottom
                            values: []
                        visual:
                            areaOpacity: 0
                            connectNulls: false
                            display: line
                            lineWidth: 1
                        yAxis:
                            format:
                                unit: seconds
                queries:
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: ${datasource}
                                query: histogram_quantile(0.99, sum by(le) (rate(vllm:time_to_first_token_seconds_bucket{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])))
                                seriesNameFormat: P99
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: ${datasource}
                                query: histogram_quantile(0.95, sum by(le) (rate(vllm:time_to_first_token_seconds_bucket{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])))
                                seriesNameFormat: P95
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: ${datasource}
                                query: histogram_quantile(0.9, sum by(le) (rate(vllm:time_to_first_token_seconds_bucket{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])))
                                seriesNameFormat: P90
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: ${datasource}
                                query: histogram_quantile(0.5, sum by(le) (rate(vllm:time_to_first_token_seconds_bucket{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])))
                                seriesNameFormat: P50
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: ${datasource}
                                query: |-
                                    rate(vllm:time_to_first_token_seconds_sum{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[$__rate_interval])
                                    /
                                    rate(vllm:time_to_first_token_seconds_count{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[$__rate_interval])
                                seriesNameFormat: Average
        "1_5":
            kind: Panel
            spec:
                display:
                    name: Time Per Output Token Latency
                    description: |-
                        The average time taken to generate each output token.

                        histogram_quantile(0.99, ...):

                        Computes the 99th percentile (P99) latency from the histogram buckets.

                        This means 99% of requests had a latency less than or equal to the returned value.

                        and so on..
                plugin:
                    kind: TimeSeriesChart
                    spec:
                        legend:
                            mode: list
                            position: bottom
                            values: []
                        visual:
                            areaOpacity: 0
                            connectNulls: false
                            display: line
                            lineWidth: 1
                        yAxis:
                            format:
                                unit: seconds
                queries:
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: ${datasource}
                                query: histogram_quantile(0.99, sum by(le) (rate(vllm:time_per_output_token_seconds_bucket{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])))
                                seriesNameFormat: P99
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: ${datasource}
                                query: histogram_quantile(0.95, sum by(le) (rate(vllm:time_per_output_token_seconds_bucket{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])))
                                seriesNameFormat: P95
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: ${datasource}
                                query: histogram_quantile(0.9, sum by(le) (rate(vllm:time_per_output_token_seconds_bucket{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])))
                                seriesNameFormat: P90
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: ${datasource}
                                query: histogram_quantile(0.5, sum by(le) (rate(vllm:time_per_output_token_seconds_bucket{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])))
                                seriesNameFormat: P50
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: ${datasource}
                                query: |-
                                    rate(vllm:time_per_output_token_seconds_sum{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])
                                    /
                                    rate(vllm:time_per_output_token_seconds_count{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])
                                seriesNameFormat: Mean
        "1_6":
            kind: Panel
            spec:
                display:
                    name: Total Request Count
                    description: |-
                        Total Requests and the end reason, that can be:

                        Length: The request ended because it reached the maximum token limit set for the model inference.

                        Stop: The request is completed naturally based on the model's output or a stop condition (e.g., end of a sentence or token completion).
                plugin:
                    kind: TimeSeriesChart
                    spec:
                        legend:
                            mode: list
                            position: bottom
                            values: []
                        visual:
                            areaOpacity: 0
                            connectNulls: false
                            display: line
                            lineWidth: 1
                        yAxis:
                            format:
                                unit: decimal
                            max: 100
                            min: 0
                queries:
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: ${datasource}
                                query: vllm:request_success_total{finished_reason="length",namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}
                                seriesNameFormat: 'Finished Reason: Length'
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: ${datasource}
                                query: vllm:request_success_total{finished_reason="stop",namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}
                                seriesNameFormat: 'Finished Reason: Stopped'
        "1_7":
            kind: Panel
            spec:
                display:
                    name: Prompt Token Throughput / Generation Throughput
                    description: Number of tokens processed per second. Tracks the speed of processing prompt tokens, which is essential for LLM optimization.
                plugin:
                    kind: TimeSeriesChart
                    spec:
                        legend:
                            mode: list
                            position: bottom
                            values: []
                        visual:
                            areaOpacity: 0
                            connectNulls: false
                            display: line
                            lineWidth: 1
                queries:
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: ${datasource}
                                query: rate(vllm:prompt_tokens_total{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])
                                seriesNameFormat: Prompt Tokens/Sec
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: ${datasource}
                                query: rate(vllm:generation_tokens_total{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])
                                seriesNameFormat: Generation Tokens/Sec
        "1_8":
            kind: Panel
            spec:
                display:
                    name: Total Generated Tokens
                    description: Total of generated tokens of the given period range
                plugin:
                    kind: GaugeChart
                    spec:
                        calculation: last-number
                        format:
                            unit: decimal
                        thresholds:
                            steps:
                                - color: green
                                  value: 0
                queries:
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: ${datasource}
                                query: sum(vllm:generation_tokens_total{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"})
                                seriesNameFormat: '{{verb}}'
        "1_9":
            kind: Panel
            spec:
                display:
                    name: Requests in Queue Time - WIP
                    description: |-
                        Indicates potential system overload or scheduling inefficiencies.
                        Usually empty, depending on the load.

                        Not currently available in the vLLM Runtime version used by RHOAI
                plugin:
                    kind: TimeSeriesChart
                    spec:
                        legend:
                            mode: list
                            position: bottom
                            values: []
                        visual:
                            areaOpacity: 0
                            connectNulls: false
                            display: line
                            lineWidth: 1
                queries:
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: edx8memhpd9tsa
                                query: rate(vllm:request_queue_time_seconds_sum{model_name="$model_name"}[$__rate_interval])
        "1_10":
            kind: Panel
            spec:
                display:
                    name: Inference Time - WIP
                    description: |-
                        Tracks the time spent in model inference, offering insights into processing efficiency.
                        Usually empty, depending on the load.

                        Not currently available in the vLLM Runtime version used by RHOAI
                plugin:
                    kind: TimeSeriesChart
                    spec:
                        legend:
                            mode: list
                            position: bottom
                            values: []
                        visual:
                            areaOpacity: 0
                            connectNulls: false
                            display: line
                            lineWidth: 1
                queries:
                    - kind: TimeSeriesQuery
                      spec:
                        plugin:
                            kind: PrometheusTimeSeriesQuery
                            spec:
                                datasource:
                                    kind: PrometheusDatasource
                                    name: edx8memhpd9tsa
                                query: rate(vllm:request_inference_time_seconds{namespace="$namespace", pod=~"$model_name.*", model_name="$model_name"}[5m])
    layouts:
        - kind: Grid
          spec:
            display:
                title: Nvidia Accelerator
                collapse:
                    open: true
            items:
                - x: 0
                  "y": 1
                  width: 13
                  height: 8
                  content:
                    $ref: '#/spec/panels/0_0'
                - x: 13
                  "y": 1
                  width: 6
                  height: 8
                  content:
                    $ref: '#/spec/panels/0_1'
                - x: 0
                  "y": 9
                  width: 19
                  height: 8
                  content:
                    $ref: '#/spec/panels/0_2'
                - x: 0
                  "y": 17
                  width: 13
                  height: 8
                  content:
                    $ref: '#/spec/panels/0_3'
                - x: 13
                  "y": 17
                  width: 6
                  height: 8
                  content:
                    $ref: '#/spec/panels/0_4'
                - x: 0
                  "y": 25
                  width: 19
                  height: 8
                  content:
                    $ref: '#/spec/panels/0_5'
                - x: 0
                  "y": 33
                  width: 19
                  height: 9
                  content:
                    $ref: '#/spec/panels/0_6'
        - kind: Grid
          spec:
            display:
                title: vLLM Metrics
                collapse:
                    open: true
            items:
                - x: 0
                  "y": 43
                  width: 12
                  height: 8
                  content:
                    $ref: '#/spec/panels/1_0'
                - x: 12
                  "y": 43
                  width: 12
                  height: 8
                  content:
                    $ref: '#/spec/panels/1_1'
                - x: 0
                  "y": 51
                  width: 12
                  height: 8
                  content:
                    $ref: '#/spec/panels/1_2'
                - x: 12
                  "y": 51
                  width: 12
                  height: 8
                  content:
                    $ref: '#/spec/panels/1_3'
                - x: 0
                  "y": 59
                  width: 12
                  height: 8
                  content:
                    $ref: '#/spec/panels/1_4'
                - x: 12
                  "y": 59
                  width: 12
                  height: 8
                  content:
                    $ref: '#/spec/panels/1_5'
                - x: 0
                  "y": 67
                  width: 12
                  height: 8
                  content:
                    $ref: '#/spec/panels/1_6'
                - x: 12
                  "y": 67
                  width: 12
                  height: 8
                  content:
                    $ref: '#/spec/panels/1_7'
                - x: 0
                  "y": 75
                  width: 6
                  height: 8
                  content:
                    $ref: '#/spec/panels/1_8'
                - x: 6
                  "y": 75
                  width: 6
                  height: 3
                  content:
                    $ref: '#/spec/panels/1_9'
                - x: 6
                  "y": 78
                  width: 6
                  height: 3
                  content:
                    $ref: '#/spec/panels/1_10'
    duration: 1h

