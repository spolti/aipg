apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  annotations:
    opendatahub.io/template-display-name: ServingRuntime for vLLM | Topsail
  labels:
    opendatahub.io/dashboard: "true"
  name: granite-3-2-2b-instruct
spec:
  builtInAdapter:
    modelLoadingTimeoutMillis: 90000
  containers:
  - args:
    - --model=ibm-granite/granite-3.2-2b-instruct
    - --download-dir=/models-cache
    - --port=8080
    - --max-model-len=16384
    - --tensor-parallel-size=1
    env:
    - name: HF_HUB_OFFLINE
      value: "0"
    - name: HF_HOME
      value: /tmp/hf_home
    image: "quay.io/modh/vllm:rhoai-2.19-cuda"
    name: kserve-container
    ports:
    - containerPort: 8080
      protocol: TCP
  multiModel: false
  supportedModelFormats:
  - autoSelect: true
    name: pytorch
---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    serving.kserve.io/deploymentMode: RawDeployment
    serving.kserve.io/enable-prometheus-scraping: "true"
  labels:
    opendatahub.io/dashboard: "true"
  name: granite-3-2-2b-instruct
spec:
  predictor:
    minReplicas: 1
    model:
      resources:
          limits:
            nvidia.com/gpu: "1"
          requests:
            nvidia.com/gpu: "1"
            cpu: "2"
            memory: "16Gi"
      runtime: granite-3-2-2b-instruct
      modelFormat:
        name: pytorch
    serviceAccountName: sa