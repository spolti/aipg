apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata: 
  name: vllm-gpt2-openai
  namespace: vllm-gpt2
spec: 
  predictor: 
    model: 
      runtime: kserve-vllm
      name: kserve-container
      modelFormat: 
        name: vLLM
      storageUri: "pvc://vlmm-gpt2-claim"
